@article{10.1145/2842602,
author = {Drineas, Petros and Mahoney, Michael W.},
title = {RandNLA: randomized numerical linear algebra},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/2842602},
doi = {10.1145/2842602},
abstract = {Randomization offers new benefits for large-scale linear algebra computations.},
journal = {Commun. ACM},
month = may,
pages = {80–90},
numpages = {11}
}

@article{doi:10.1137/090771806,
author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
title = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
journal = {SIAM Review},
volume = {53},
number = {2},
pages = {217-288},
year = {2011},
doi = {10.1137/090771806},
URL = {https://doi.org/10.1137/090771806},
eprint = {https://doi.org/10.1137/090771806},
abstract = { Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed—either explicitly or implicitly—to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an \$m \times n\$ matrix. (i) For a dense input matrix, randomized algorithms require \$\bigO(mn \log(k))\$ floating-point operations (flops) in contrast to \$ \bigO(mnk)\$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to \$\bigO(k)\$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data. }
}
@article{AKRITAS200415,
title = {Applications of singular-value decomposition (SVD)},
journal = {Mathematics and Computers in Simulation},
volume = {67},
number = {1},
pages = {15-31},
year = {2004},
note = {Applications of Computer Algebra in Science, Engineering, Simulation and Special Software},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2004.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S037847540400151X},
author = {Alkiviadis G. Akritas and Gennadi I. Malaschonok},
keywords = {Applications, Singular-value decompositions, Hanger, Stretcher, Aligner},
abstract = {Let A be an m×n matrix with m≥n. Then one form of the singular-value decomposition of A is A=UTΣV,where U and V are orthogonal and Σ is square diagonal. That is, UUT=Irank(A), VVT=Irank(A), U is rank(A)×m, V is rank(A)×n and Σ=σ10⋯000σ2⋯00⋮⋮⋱⋮⋮00⋯σrank(A)−1000⋯0σrank(A)is a rank(A)×rank(A) diagonal matrix. In addition σ1≥σ2≥⋯≥σrank(A)>0. The σi’s are called the singular values of A and their number is equal to the rank of A. The ratio σ1/σrank(A) can be regarded as a condition number of the matrix A. It is easily verified that the singular-value decomposition can be also written as A=UTΣV=∑i=1rank(A)σiuiTvi.The matrix uiTvi is the outer product of the i-th row of U with the corresponding row of V. Note that each of these matrices can be stored using only m+n locations rather than mn locations. Using both forms presented above—and following Jerry Uhl’s beautiful approach in the Calculus and Mathematica book series [Matrices, Geometry & Mathematica, Math Everywhere Inc., 1999]—we show how SVD can be used as a tool for teaching Linear Algebra geometrically, and then apply it in solving least-squares problems and in data compression. In this paper we used the Computer Algebra system Mathematica to present a purely numerical problem. In general, the use of Computer Algebra systems has greatly influenced the teaching of mathematics, allowing students to concentrate on the main ideas and to visualize them.}
}
@article{WANG2023102182,
title = {Enhancing the SVD compression losslessly},
journal = {Journal of Computational Science},
volume = {74},
pages = {102182},
year = {2023},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2023.102182},
url = {https://www.sciencedirect.com/science/article/pii/S1877750323002429},
author = {Huiwen Wang and Yanwen Zhang and Jichang Zhao},
keywords = {Singular value decomposition, Givens transformation, Lossless compression, Orthogonal column matrix, E-SVD},
abstract = {Orthonormality is the foundation of a number of matrix decomposition methods. For example, Singular Value Decomposition (SVD) implements the compression by factoring a matrix with orthonormal parts and is pervasively utilized in various fields. Orthonormality, however, inherently includes constraints that would induce redundant information, preventing SVD from deeper compression and even making it frustrated as the data fidelity is strictly required. In this paper, we theoretically prove that these redundances resulted by orthonormality can be completely eliminated in a lossless manner. An enhanced version of SVD, namely E-SVD, is accordingly established to losslessly and quickly release constraints and recover the orthonormal parts in SVD. According to our theory, advantages of E-SVD over SVD become increasingly evident with the rising requirement of data fidelity. In particular, E-SVD will reduce 25% storage units as SVD reaches its limitation and fails to compress data. Empirical evidences from remote sensing and internet of things justify our theory by demonstrating the consistent compression superiority of E-SVD over SVD, and the additional calculation cost of E-SVD is of the same magnitude with SVD. Digital image compression experiment with typical size 375 × 500 also shows that E-SVD needs the least storage space as compared to alternative compression solutions when the rank is sufficient to maintain a good visual quality. The presented theory sheds insightful lights on the constraint solution in orthonormal matrices and E-SVD, guaranteed by which will profoundly enhance the SVD-based compression in the context of explosive growth in both data acquisition and fidelity levels.}
}
@article{BRYT2008270,
title = {Compression of facial images using the K-SVD algorithm},
journal = {Journal of Visual Communication and Image Representation},
volume = {19},
number = {4},
pages = {270-282},
year = {2008},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2008.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1047320308000254},
author = {Ori Bryt and Michael Elad},
keywords = {Image compression, Sparse representations, Redundancy, K-SVD, OMP, Facial images, PCA, JPEG, JPEG2000, VQ},
abstract = {The use of sparse representations in signal and image processing is gradually increasing in the past several years. Obtaining an overcomplete dictionary from a set of signals allows us to represent them as a sparse linear combination of dictionary atoms. Pursuit algorithms are then used for signal decomposition. A recent work introduced the K-SVD algorithm, which is a novel method for training overcomplete dictionaries that lead to sparse signal representation. In this work we propose a new method for compressing facial images, based on the K-SVD algorithm. We train K-SVD dictionaries for predefined image patches, and compress each new image according to these dictionaries. The encoding is based on sparse coding of each image patch using the relevant trained dictionary, and the decoding is a simple reconstruction of the patches by linear combination of atoms. An essential pre-process stage for this method is an image alignment procedure, where several facial features are detected and geometrically warped into a canonical spatial location. We present this new method, analyze its results and compare it to several competing compression techniques.}
}
@article{
doi:10.1073/pnas.0803205106,
author = {Michael W. Mahoney  and Petros Drineas },
title = {CUR matrix decompositions for improved data analysis},
journal = {Proceedings of the National Academy of Sciences},
volume = {106},
number = {3},
pages = {697-702},
year = {2009},
doi = {10.1073/pnas.0803205106},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0803205106},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0803205106},
abstract = {Principal components analysis and, more generally, the Singular Value Decomposition are fundamental data analysis tools that express a data matrix in terms of a sequence of orthogonal or uncorrelated vectors of decreasing importance. Unfortunately, being linear combinations of up to all the data points, these vectors are notoriously difficult to interpret in terms of the data and processes generating the data. In this article, we develop CUR matrix decompositions for improved data analysis. CUR decompositions are low-rank matrix decompositions that are explicitly expressed in terms of a small number of actual columns and/or actual rows of the data matrix. Because they are constructed from actual data elements, CUR decompositions are interpretable by practitioners of the field from which the data are drawn (to the extent that the original data are). We present an algorithm that preferentially chooses columns and rows that exhibit high “statistical leverage” and, thus, in a very precise statistical sense, exert a disproportionately large “influence” on the best low-rank fit of the data matrix. By selecting columns and rows in this manner, we obtain improved relative-error and constant-factor approximation guarantees in worst-case analysis, as opposed to the much coarser additive-error guarantees of prior work. In addition, since the construction involves computing quantities with a natural and widely studied statistical interpretation, we can leverage ideas from diagnostic regression analysis to employ these matrix decompositions for exploratory data analysis.}}

@article{Erichson_2019,
   title={Randomized Matrix Decompositions Using R},
   volume={89},
   ISSN={1548-7660},
   url={http://dx.doi.org/10.18637/jss.v089.i11},
   DOI={10.18637/jss.v089.i11},
   number={11},
   journal={Journal of Statistical Software},
   publisher={Foundation for Open Access Statistic},
   author={Erichson, N. Benjamin and Voronin, Sergey and Brunton, Steven L. and Kutz, J. Nathan},
   year={2019} }


